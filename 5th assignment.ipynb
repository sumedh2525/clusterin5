{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12db976-0aec-454e-adaa-4485b011dcaa",
   "metadata": {},
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
    "\n",
    "\n",
    " Contingency Matrix:\n",
    "A contingency matrix, also known as a confusion matrix, is a table used in classification to evaluate the performance of a model. It compares the predicted classes of a model with the true classes of the data. The matrix has four entries: true positive (TP), false positive (FP), true negative (TN), and false negative (FN). It provides a snapshot of how well a classification model is performing and helps in the calculation of various performance metrics such as precision, recall, F1 score, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759e327-caac-412d-9dbe-236691efc588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad12e7b-b8e1-4875-a1b4-a141c53c5ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd52b486-1e31-4648-9d83-6bffdf94a573",
   "metadata": {},
   "source": [
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in\n",
    "certain situations?\n",
    "\n",
    "Pair Confusion Matrix:\n",
    "A pair confusion matrix is a specialized version of a confusion matrix used in binary classification tasks. In a regular confusion matrix, you have entries for true positive, false positive, true negative, and false negative. In a pair confusion matrix, these entries are broken down further into pairs of correctly and incorrectly classified instances. This can be useful in situations where understanding the specifics of pair-wise classification performance is crucial, such as in certain medical diagnostic tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9299d70-4e6e-4973-ae24-be5d7e6df850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5cb07-cde7-4e08-a5a6-616ca1c0fea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f13439fc-968c-4658-95b2-8fb37978d94d",
   "metadata": {},
   "source": [
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically\n",
    "\n",
    "\n",
    "Extrinsic Measure in NLP:\n",
    "In natural language processing (NLP), an extrinsic measure is an evaluation metric that assesses the performance of a language model based on its contribution to a specific downstream task. For example, if the language model is used for sentiment analysis, the accuracy or F1 score on sentiment classification tasks would be an extrinsic measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde7419-ca06-422e-851f-a5992c9f6498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec4c14-0b6c-477e-9b78-c3ac9de9632d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52518cb4-71c4-44cc-9b93-b35aa71e67ff",
   "metadata": {},
   "source": [
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an\n",
    "extrinsic measure?\n",
    "\n",
    "\n",
    " Intrinsic Measure in ML:\n",
    "An intrinsic measure in machine learning evaluates the performance of a model based on its inherent properties, independent of any specific application or downstream task. For instance, perplexity is an intrinsic measure commonly used in language modeling to assess how well a model predicts a sequence of words. Unlike extrinsic measures, intrinsic measures focus on model characteristics rather than application-specific performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43634afd-f557-4eca-8d35-8568420b57ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee09b27a-50ef-47ec-80f3-be349af420f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8c91e67-f877-4d79-a8bc-31063084e331",
   "metadata": {},
   "source": [
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify\n",
    "strengths and weaknesses of a model?\n",
    "\n",
    "\n",
    "Confusion Matrix in ML:\n",
    "The confusion matrix in machine learning is a table that helps evaluate the performance of a classification model. It shows the count of true positive, false positive, true negative, and false negative predictions. By analyzing these values, one can calculate various performance metrics, identify areas of improvement, and understand the strengths and weaknesses of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16593072-d6d4-4793-8341-c6f51668b6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b82d8-4428-4780-8af7-cd22f928d7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03e0ec2b-8985-4f61-9e30-d3bcc4a4e1f0",
   "metadata": {},
   "source": [
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised\n",
    "learning algorithms, and how can they be interpreted?\n",
    "\n",
    "\n",
    "Intrinsic Measures for Unsupervised Learning:\n",
    "In unsupervised learning, common intrinsic measures include silhouette score for clustering algorithms and Daviesâ€“Bouldin index. These measures provide insights into the quality of clusters formed by the algorithm. Silhouette score, for example, quantifies how well-separated clusters are and ranges from -1 to 1, with higher values indicating better-defined clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654c267-78c3-4970-ba11-8eaaa7c973a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1958ffa-07f3-4c8d-b21a-44822df4cdf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efde7d7a-935f-4267-b26e-0ce3cb8df20f",
   "metadata": {},
   "source": [
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and\n",
    "how can these limitations be addressed?\n",
    "\n",
    "\n",
    " Limitations of Accuracy in Classification:\n",
    "\n",
    "Imbalanced Datasets: Accuracy may be misleading when dealing with imbalanced datasets where one class significantly outnumbers the other.\n",
    "Misleading Implications: Accuracy does not distinguish between types of errors (false positives vs. false negatives).\n",
    "Threshold Sensitivity: The choice of classification threshold can impact accuracy, especially in models with probabilistic outputs.\n",
    "Application Context: Accuracy may not reflect the model's performance in specific application contexts. Precision, recall, and F1 score offer a more nuanced evaluation in such cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24123898-d9da-4ca4-8d45-163ec98dd204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
